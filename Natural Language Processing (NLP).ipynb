{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab84e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words dimension = 362891\n",
      "all_embeddings dimensions: (362891, 300)\n",
      "/c/de/abh√§ngen\n",
      "all_words dimensions: 150875\n",
      "all_embeddings dimensions: (150875, 300)\n",
      "activated_carbon\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "#to open mini.h5 file, r represents read only\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"All words dimension = {0}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions: {0}\".format(all_embeddings.shape))\n",
    "\n",
    "print(all_words[133])\n",
    "\n",
    "#To extract just the English Letter\n",
    "#checks if each word starts with the prefix '/c/en/' using the startswith method. If it does, it extracts the substring starting from the 6th character and adds it to the english_words list.\n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embeddings = all_embeddings[english_word_indices]\n",
    "\n",
    "print(\"all_words dimensions: {0}\".format(len(english_words)))\n",
    "print(\"all_embeddings dimensions: {0}\".format(english_embeddings.shape))\n",
    "\n",
    "print(english_words[1337])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd8dd9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0000001\n",
      "cat\tfeline\t 0.8199548\n",
      "cat\tdog\t 0.590724\n",
      "cat\tmoo\t 0.0039538303\n",
      "cat\tfreeze\t -0.030225191\n",
      "antonyms\topposites\t 0.3941065\n",
      "antonyms\tsynonyms\t 0.46883982\n",
      "['cat', 'humane_society', 'kitten', 'feline', 'colocolo', 'cats', 'kitty', 'maine_coon', 'housecat', 'sharp_teeth']\n",
      "['dog', 'dogs', 'wire_haired_dachshund', 'doggy_paddle', 'lhasa_apso', 'good_friend', 'puppy_dog', 'bichon_frise', 'woof_woof', 'golden_retrievers']\n",
      "['duke', 'dukes', 'duchess', 'duchesses', 'ducal', 'dukedom', 'duchy', 'voivode', 'princes', 'prince']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#To normalize the embedding vectors\n",
    "#norms of the embedding vectors represent their magnitudes or lengths\n",
    "norms = np.linalg.norm(english_embeddings, axis=1)\n",
    "#Normalization scales the embedding vectors to have unit length. It is performed by dividing each embedding vector by its corresponding norm\n",
    "normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])\n",
    "\n",
    "#This line creates a dictionary called index that maps each word in the english_words list to its corresponding index\n",
    "index = {word: i for i, word in enumerate(english_words)}\n",
    "\n",
    "def similarity_score(w1, w2):\n",
    "    #calculates the similarity score between two words, w1 and w2\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score\n",
    "\n",
    "def closest_to_vector(v, n):\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words = map(lambda i: english_words[i], reversed(np.argsort(all_scores)))\n",
    "    return [next(best_words) for _ in range(n)]\n",
    "\n",
    "def most_similar(w, n):\n",
    "    return closest_to_vector(normalized_embeddings[index[w], :], n)\n",
    "\n",
    "def solve_analogy(a1, b1, a2):\n",
    "    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]\n",
    "    return closest_to_vector(b2, 1)\n",
    "\n",
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))\n",
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonyms\\topposites\\t', similarity_score('antonym', 'opposite'))\n",
    "print('antonyms\\tsynonyms\\t', similarity_score('antonym', 'synonym'))\n",
    "\n",
    "print(most_similar('cat', 10))\n",
    "print(most_similar('dog', 10))\n",
    "print(most_similar('duke', 10))\n",
    "\n",
    "print(solve_analogy(\"man\", \"brother\", \"woman\"))\n",
    "print(solve_analogy(\"man\", \"husband\", \"woman\"))\n",
    "print(solve_analogy(\"spain\", \"madrid\", \"france\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44e8d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "remove_punct = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words if w in index]\n",
    "\n",
    "    # Take the mean of the embeddings\n",
    "    x = tf.reduce_mean(tf.convert_to_tensor(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "with open(\"movie-simple.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2792466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 0.6860820055007935 \t Acc: 0.5460000038146973\n",
      "Epoch: 10 \t Loss: 0.6760073900222778 \t Acc: 0.5519999861717224\n",
      "Epoch: 20 \t Loss: 0.6886350512504578 \t Acc: 0.5519999861717224\n",
      "Epoch: 30 \t Loss: 0.665454626083374 \t Acc: 0.5540000200271606\n",
      "Epoch: 40 \t Loss: 0.6349549293518066 \t Acc: 0.5889999866485596\n",
      "Epoch: 50 \t Loss: 0.6492026448249817 \t Acc: 0.6140000224113464\n",
      "Epoch: 60 \t Loss: 0.6269775629043579 \t Acc: 0.6779999732971191\n",
      "Epoch: 70 \t Loss: 0.5658120512962341 \t Acc: 0.7570000290870667\n",
      "Epoch: 80 \t Loss: 0.5489456653594971 \t Acc: 0.8090000152587891\n",
      "Epoch: 90 \t Loss: 0.4851008653640747 \t Acc: 0.8659999966621399\n",
      "Epoch: 100 \t Loss: 0.40472617745399475 \t Acc: 0.8859999775886536\n",
      "Epoch: 110 \t Loss: 0.324491411447525 \t Acc: 0.9049999713897705\n",
      "Epoch: 120 \t Loss: 0.24773387610912323 \t Acc: 0.9150000214576721\n",
      "Epoch: 130 \t Loss: 0.268240749835968 \t Acc: 0.9150000214576721\n",
      "Epoch: 140 \t Loss: 0.2238071858882904 \t Acc: 0.9269999861717224\n",
      "Epoch: 150 \t Loss: 0.1866254210472107 \t Acc: 0.9369999766349792\n",
      "Epoch: 160 \t Loss: 0.16890040040016174 \t Acc: 0.9440000057220459\n",
      "Epoch: 170 \t Loss: 0.18282848596572876 \t Acc: 0.9480000138282776\n",
      "Epoch: 180 \t Loss: 0.13348136842250824 \t Acc: 0.9470000267028809\n",
      "Epoch: 190 \t Loss: 0.15364889800548553 \t Acc: 0.9470000267028809\n",
      "Epoch: 200 \t Loss: 0.11074766516685486 \t Acc: 0.9599999785423279\n",
      "Epoch: 210 \t Loss: 0.0808468759059906 \t Acc: 0.9620000123977661\n",
      "Epoch: 220 \t Loss: 0.1222425103187561 \t Acc: 0.9620000123977661\n",
      "Epoch: 230 \t Loss: 0.09648792445659637 \t Acc: 0.9649999737739563\n",
      "Epoch: 240 \t Loss: 0.1122899278998375 \t Acc: 0.968999981880188\n",
      "Final accuracy: 0.9586374759674072\n",
      "exciting tf.Tensor([[0.9999632]], shape=(1, 1), dtype=float32)\n",
      "hated tf.Tensor([[1.2759547e-06]], shape=(1, 1), dtype=float32)\n",
      "boring tf.Tensor([[1.846753e-05]], shape=(1, 1), dtype=float32)\n",
      "loved tf.Tensor([[0.9999999]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches * batch_size], dataset[train_batches * batch_size:]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Metric for accuracy\n",
    "accuracy_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train)\n",
    "    for batch_start in range(0, len(train), batch_size):\n",
    "        batch_data = train[batch_start:batch_start + batch_size]\n",
    "        reviews = np.array([sample['x'] for sample in batch_data])\n",
    "        labels = np.array([sample['y'] for sample in batch_data])\n",
    "        labels = labels.reshape(-1, 1)  # Reshape labels to match logits shape\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(reviews)\n",
    "            loss_value = loss_fn(labels, logits)\n",
    "        \n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        accuracy_metric.update_state(labels, tf.math.sigmoid(logits))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: {0} \\t Loss: {1} \\t Acc: {2}\".format(epoch, loss_value, accuracy_metric.result()))\n",
    "    \n",
    "    accuracy_metric.reset_states()\n",
    "\n",
    "# Evaluation\n",
    "test_reviews = np.array([sample['x'] for sample in test])\n",
    "test_labels = np.array([sample['y'] for sample in test])\n",
    "test_labels = test_labels.reshape(-1, 1)  # Reshape test labels\n",
    "logits = model(test_reviews)\n",
    "test_predictions = tf.math.sigmoid(logits)\n",
    "test_accuracy = accuracy_metric(test_labels, test_predictions)\n",
    "print(\"Final accuracy: {0}\".format(test_accuracy))\n",
    "\n",
    "# Check some words\n",
    "words_to_test = [\"exciting\", \"hated\", \"boring\", \"loved\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    word_embedding = normalized_embeddings[index[word]].reshape(1, 300)\n",
    "    word_probabilities = tf.math.sigmoid(model(word_embedding))\n",
    "    print(word, word_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062a79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce1ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
